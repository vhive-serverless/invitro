Name:         activator-598c797c7-8nf52
Namespace:    knative-serving
Priority:     0
Node:         node-1.one-worker.faas-sched.emulab.net/155.98.36.106
Start Time:   Sun, 01 May 2022 23:15:39 -0600
Labels:       app=activator
              app.kubernetes.io/component=activator
              app.kubernetes.io/name=knative-serving
              app.kubernetes.io/version=1.3.0
              pod-template-hash=598c797c7
              role=activator
              serving.knative.dev/release=v1.3.0
Annotations:  cluster-autoscaler.kubernetes.io/safe-to-evict: false
              cni.projectcalico.org/podIP: 192.168.9.160/32
              cni.projectcalico.org/podIPs: 192.168.9.160/32
              kubectl.kubernetes.io/restartedAt: 2022-05-01T16:45:50-06:00
Status:       Running
IP:           192.168.9.160
IPs:
  IP:           192.168.9.160
Controlled By:  ReplicaSet/activator-598c797c7
Containers:
  activator:
    Container ID:   containerd://ba98e771e06f33eea00a4a4dbfebb033720e8580f9bb02ee622668f59a07b769
    Image:          gcr.io/knative-releases/knative.dev/serving/cmd/activator@sha256:624ad8df549de9192e7b0d2f596c08c5996f678b25ff94688464412890126bb1
    Image ID:       gcr.io/knative-releases/knative.dev/serving/cmd/activator@sha256:624ad8df549de9192e7b0d2f596c08c5996f678b25ff94688464412890126bb1
    Ports:          9090/TCP, 8008/TCP, 8012/TCP, 8013/TCP
    Host Ports:     0/TCP, 0/TCP, 0/TCP, 0/TCP
    State:          Running
      Started:      Sun, 01 May 2022 23:30:02 -0600
    Last State:     Terminated
      Reason:       OOMKilled
      Exit Code:    137
      Started:      Sun, 01 May 2022 23:26:16 -0600
      Finished:     Sun, 01 May 2022 23:27:09 -0600
    Ready:          True
    Restart Count:  6
    Limits:
      cpu:     1
      memory:  600Mi
    Requests:
      cpu:      300m
      memory:   60Mi
    Liveness:   http-get http://:8012/ delay=15s timeout=1s period=10s #success=1 #failure=12
    Readiness:  http-get http://:8012/ delay=0s timeout=1s period=5s #success=1 #failure=5
    Environment:
      GOGC:                       500
      POD_NAME:                   activator-598c797c7-8nf52 (v1:metadata.name)
      POD_IP:                      (v1:status.podIP)
      SYSTEM_NAMESPACE:           knative-serving (v1:metadata.namespace)
      CONFIG_LOGGING_NAME:        config-logging
      CONFIG_OBSERVABILITY_NAME:  config-observability
      METRICS_DOMAIN:             knative.dev/internal/serving
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-77z7k (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-77z7k:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age                   From               Message
  ----     ------            ----                  ----               -------
  Warning  FailedScheduling  28m                   default-scheduler  0/2 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 1 node(s) had taint {node.kubernetes.io/disk-pressure: }, that the pod didn't tolerate.
  Warning  FailedScheduling  18m (x1 over 26m)     default-scheduler  0/2 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 1 node(s) had taint {node.kubernetes.io/disk-pressure: }, that the pod didn't tolerate.
  Warning  FailedScheduling  14m (x1 over 20m)     default-scheduler  0/2 nodes are available: 1 Too many pods, 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate.
  Normal   Scheduled         17m                   default-scheduler  Successfully assigned knative-serving/activator-598c797c7-8nf52 to node-1.one-worker.faas-sched.emulab.net
  Normal   Pulling           17m                   kubelet            Pulling image "gcr.io/knative-releases/knative.dev/serving/cmd/activator@sha256:624ad8df549de9192e7b0d2f596c08c5996f678b25ff94688464412890126bb1"
  Normal   Pulled            17m                   kubelet            Successfully pulled image "gcr.io/knative-releases/knative.dev/serving/cmd/activator@sha256:624ad8df549de9192e7b0d2f596c08c5996f678b25ff94688464412890126bb1" in 2.210575945s
  Warning  Unhealthy         15m                   kubelet            Readiness probe failed: Get "http://192.168.9.160:8012/": read tcp 155.98.36.106:38899->192.168.9.160:8012: read: connection reset by peer
  Warning  Unhealthy         15m                   kubelet            Liveness probe failed: Get "http://192.168.9.160:8012/": read tcp 155.98.36.106:38901->192.168.9.160:8012: read: connection reset by peer
  Normal   Created           14m (x3 over 17m)     kubelet            Created container activator
  Normal   Started           14m (x3 over 17m)     kubelet            Started container activator
  Warning  Unhealthy         13m                   kubelet            Readiness probe failed: Get "http://192.168.9.160:8012/": dial tcp 192.168.9.160:8012: connect: connection refused
  Normal   Pulled            11m (x4 over 15m)     kubelet            Container image "gcr.io/knative-releases/knative.dev/serving/cmd/activator@sha256:624ad8df549de9192e7b0d2f596c08c5996f678b25ff94688464412890126bb1" already present on machine
  Warning  BackOff           7m58s (x15 over 14m)  kubelet            Back-off restarting failed container
